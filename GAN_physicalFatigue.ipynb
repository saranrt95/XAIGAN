{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_physicalFatigue.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpUSC-N3mXf-",
        "outputId": "45c31de2-e5cf-4cd6-8529-00df0c5210ff"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Activation, Dense, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import sys\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZVO135vnBBS"
      },
      "source": [
        "# Build discriminator and generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvdwyJ1tnIyg"
      },
      "source": [
        "def build_generator(inputs, n_features, activation='sigmoid',labels=None):\n",
        "    # network parameters : sizes of the hidden layers\n",
        "    layers_size=[128, 64, 32, 1]\n",
        "    if labels is not None:\n",
        "      inputs = [inputs, labels]\n",
        "      x = concatenate(inputs, axis=1)\n",
        "    else:\n",
        "        # default input is just 100-dim noise (z-code)\n",
        "        x = inputs\n",
        "    x = Dense(n_features*layers_size[0])(x)\n",
        "    x = Reshape((n_features,layers_size[0]))(x)\n",
        "\n",
        "    for units in layers_size:\n",
        "      x = BatchNormalization()(x)\n",
        "      x = Activation('relu')(x)\n",
        "      x = Dense(units, bias_regularizer=regularizers.l2(1e-4), kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "\n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "    gen= Model(inputs, x, name='generator')\n",
        "    #gen.summary()\n",
        "    # generator output is the synthesized dataset\n",
        "    return gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEhKFIFnzOvC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1w46YZLnMp_"
      },
      "source": [
        "def build_discriminator(inputs, n_features, activation='sigmoid', labels=None):\n",
        "    layers_size = [32, 64, 128]# , 256, 512] #, 256\n",
        "\n",
        "    x = inputs\n",
        "    y = Dense(38)(labels) #Dense() takes the number of features\n",
        "    y = Reshape((38, 1))(y)\n",
        "    x = concatenate([x, y])\n",
        "    for units in layers_size:\n",
        "      x = LeakyReLU(alpha=0.2)(x)\n",
        "      x = Dense(units, bias_regularizer=regularizers.l2(1e-4), kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    # default output is probability that the data is real\n",
        "    outputs = Dense(1)(x)\n",
        "    if activation is not None:\n",
        "        print(activation)\n",
        "        outputs = Activation(activation)(outputs)\n",
        "\n",
        "    return Model([inputs, labels], outputs, name='discriminator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI3-_gNCnsTW"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N79fjd0Tn0vm"
      },
      "source": [
        "Frechet-Inception Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnb30FChnwHu"
      },
      "source": [
        "\n",
        "# function to compute the FID between two distributions\n",
        "def calculate_fid(act1, act2):\n",
        "  mu1 = act1.mean(axis = 0)\n",
        "  sigma1 = np.cov(act1.astype(float), rowvar = False)\n",
        "  mu2 = act2.mean(axis = 0)\n",
        "  sigma2 = np.cov(act2.astype(float), rowvar = False)\n",
        "  # calculate sum squared difference between means\n",
        "  ssdiff = np.sum((mu1-mu2)**2.0)\n",
        "  # calculate sqrt of product between cov\n",
        "  covmean = np.sqrt(np.abs(sigma1.dot(sigma2)))\n",
        "\n",
        "  fid = ssdiff + np.trace(sigma1 + sigma2 -2.0*covmean)\n",
        "\n",
        "  return fid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNgY1okFn57m"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCy6IPPln8sM"
      },
      "source": [
        "def train(models, data, y, params):\n",
        "    # the GAN models\n",
        "    generator, discriminator, adversarial = models\n",
        "    # input data and labels\n",
        "    x_train = data\n",
        "    y_train = y \n",
        "    # network parameters and runs counter r\n",
        "    batch_size, latent_size, train_steps, num_labels, model_name,r = params\n",
        "    #batch_size, latent_size, train_steps, num_labels, model_name = params\n",
        "\n",
        "    # noise vector to see how the generator output evolves during training\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
        "    # one-hot label the noise will be conditioned to\n",
        "    noise_class = np.eye(num_labels)[np.arange(0, 16) % num_labels]\n",
        "    # number of elements in train dataset\n",
        "    train_size = x_train.shape[0]\n",
        "\n",
        "    print(model_name,\n",
        "          \"Labels for generated data: \",\n",
        "          np.argmax(noise_class, axis=1))\n",
        "    \n",
        "    # uncomment and adapt the path to save training logs\n",
        "\n",
        "    #with open (\"/content/drive/MyDrive/Colab Notebooks/GAN_fatica/trainingMMH/discriminator_\"+str(train_steps)+\".csv\",\"a\") as discr_file:\n",
        "    #  discr_file.write(\"step,loss,accuracy,FID\"+\"\\n\")\n",
        "    #with open (\"/content/drive/MyDrive/Colab Notebooks/GAN_fatica/trainingMMH/adversarial_\"+str(train_steps)+\".csv\",\"a\") as adv_file:\n",
        "    #  adv_file.write(\"step,loss,accuracy\"+\"\\n\")\n",
        "    \n",
        "    # train the GAN for train_steps iterations\n",
        "    for i in range(train_steps):\n",
        "        # train the discriminator for 1 batch\n",
        "        # 1 batch of real (label=1.0) and fake data (label=0.0)\n",
        "        # randomly pick real data from dataset\n",
        "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
        "        real_data = x_train[rand_indexes]\n",
        "        # corresponding one-hot labels of real data\n",
        "        real_labels = y_train[rand_indexes]\n",
        "        # generate fake data from noise using generator\n",
        "        # generate noise using uniform distribution\n",
        "        noise = np.random.uniform(-1.0, 1.0,size=[batch_size, latent_size])                                \n",
        "        # assign random one-hot labels\n",
        "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels, batch_size, p=[0.5, 0.5])]                                                       \n",
        "        \n",
        "        # generate fake data conditioned on fake labels\n",
        "        fake_data = generator.predict([noise, fake_labels])\n",
        "        fake_data = np.reshape(fake_data,(fake_data.shape[0], fake_data.shape[1]))\n",
        "        #  real + fake data = 1 batch of train data\n",
        "        x = np.concatenate((real_data, fake_data))\n",
        "        # real + fake one-hot labels = 1 batch of train one-hot labels\n",
        "        labels = np.concatenate((real_labels, fake_labels))\n",
        "\n",
        "        # label real and fake data\n",
        "        # real data label is 1.0\n",
        "        y = np.ones([2 * batch_size, 1])\n",
        "        # fake data label is 0.0\n",
        "        y[batch_size:, :] = 0.0\n",
        "        \n",
        "        # train discriminator network, log the loss and accuracy\n",
        "                ###########\n",
        "        ### FID (measured during training iterations) ###\n",
        "        ###########\n",
        "        fid=calculate_fid(real_data,fake_data)\n",
        "\n",
        "        loss, acc = discriminator.train_on_batch([x, labels], y)\n",
        "        #salvo su file\n",
        "        #with open (\"/content/drive/MyDrive/Colab Notebooks/GAN_fatica/trainingMMH/discriminator_\"+str(train_steps)+\".csv\",\"a\") as discr_file:\n",
        "        #  discr_file.write(str(i)+\",\"+ str(loss)+\",\"+str(acc)+\",\"+str(fid)+\"\\n\")\n",
        "        log = \"%d: [discriminator loss: %f, accuracy: %f]\" % (i, loss, acc)\n",
        "        \n",
        "        # train the adversarial network for 1 batch\n",
        "        # 1 batch of fake data conditioned on fake 1-hot labels \n",
        "        # w/ label=1.0\n",
        "        # since the discriminator weights are frozen in \n",
        "        # adversarial network only the generator is trained\n",
        "        # generate noise using uniform distribution        \n",
        "        noise = np.random.uniform(-1.0,\n",
        "                                  1.0,\n",
        "                                  size=[batch_size, latent_size])\n",
        "        # assign random one-hot labels\n",
        "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n",
        "                                                          batch_size)]\n",
        "        # label fake data as real or 1.0\n",
        "        y = np.ones([batch_size, 1])\n",
        "        \n",
        "        # train the adversarial (generator) network \n",
        "        # note that unlike in discriminator training, \n",
        "        # we do not save the fake data in a variable\n",
        "        # the fake data go to the discriminator input\n",
        "        # of the adversarial for classification\n",
        "        # log the loss \n",
        "        loss, acc = adversarial.train_on_batch([noise, fake_labels], y)\n",
        "        log = \"%s [adversarial loss: %f, accuracy adv: %f]\" % (log, loss, acc)\n",
        "        #with open (\"/content/drive/MyDrive/Colab Notebooks/GAN_fatica/trainingMMH/adversarial_\"+str(train_steps)+\".csv\",\"a\") as adv_file:\n",
        "        #  adv_file.write(str(i)+\",\"+ str(loss)+\",\"+str(acc)+\"\\n\")\n",
        "        print(log)\n",
        "    \n",
        "    # save the model after training the generator\n",
        "    # the trained generator can be reloaded for \n",
        "    # future generation\n",
        "    generator.save(\"/content/drive/MyDrive/Colab Notebooks/GAN_fatica/training_MMH/\"+model_name + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTi6qSNool_A"
      },
      "source": [
        "Build and train GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DgiO9yPoyaY"
      },
      "source": [
        "# build the GAN components and train them;\n",
        "#r is the number of runs \n",
        "# train steps is fixed the number of epochs (here intended as iterations)\n",
        "\n",
        "def build_and_train_models(data, y, train_steps,r):\n",
        "\n",
        "    num_labels = np.amax(y) + 1\n",
        "    y = to_categorical(y)\n",
        "\n",
        "    input_shape = (data.shape[1],1) #columns in the data\n",
        "    label_shape = (num_labels, )#quante label\n",
        "    # CHANGE MODEL NAME (OVER OR UNDER)\n",
        "    model_name = \"fatigue_over\"+str(r)\n",
        "    # network parameters\n",
        "    latent_size = 100 #size of the noise vector\n",
        "    batch_size = 64 \n",
        "    lr = 5e-5 #learning rate \n",
        "\n",
        "    # build DISCRIMINATOR model\n",
        "    inputs = Input(shape=input_shape, name='discriminator_input')\n",
        "    labels = Input(shape=label_shape, name='class_labels')\n",
        "\n",
        "    discriminator = build_discriminator(inputs, data.shape[1], 'linear',labels)\n",
        "    # ADAM optimizer\n",
        "    optimizer = Adam(learning_rate=lr,beta_1=0.9,epsilon=1e-07)\n",
        "    discriminator.compile(loss='binary_crossentropy',\n",
        "                          optimizer=optimizer,\n",
        "                          metrics=['accuracy'])\n",
        "    discriminator.summary()\n",
        "\n",
        "    # build GENERATOR model\n",
        "    input_shape = (latent_size, )\n",
        "    inputs = Input(shape=input_shape, name='z_input')\n",
        "    generator = build_generator(inputs, data.shape[1], 'sigmoid', labels)\n",
        "    generator.summary()\n",
        "\n",
        "    # build ADVERSARIAL model = generator + discriminator\n",
        "    # freeze the weights of discriminator during adversarial training\n",
        "    discriminator.trainable = False\n",
        "    outputs = discriminator([generator([inputs, labels]), labels])\n",
        "    adversarial = Model([inputs, labels],\n",
        "                        outputs,\n",
        "                        name=model_name)\n",
        "    adversarial.compile(loss='binary_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "    adversarial.summary()\n",
        "\n",
        "    # train discriminator and adversarial networks\n",
        "    models = (generator, discriminator, adversarial)\n",
        "    params = (batch_size, latent_size, train_steps, num_labels, model_name,r)\n",
        "    train(models, data, y, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-flGDuDo8lX"
      },
      "source": [
        "# MAIN\n",
        "Import the original real dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d1v-60WpBZU"
      },
      "source": [
        "# over 40 data\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/Github XAIGAN/Real Data and Rules/MMH_over40.xlsx\")\n",
        "# under 40 data\n",
        "# data = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/Github XAIGAN/Real Data and Rules/MMH_under40.xlsx\")\n",
        "#data.head()\n",
        "targetName=\"fatiguestate1\"\n",
        "dataoutput = data[targetName]\n",
        "datadd = data\n",
        "\n",
        "datadd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4EsI2Cppx3M",
        "outputId": "9f7a15ce-997a-484e-ae66-5c3258e1df29"
      },
      "source": [
        "data = data.drop([targetName], axis=1)\n",
        "data = data.fillna(data.median())\n",
        "#print(data.head(3))\n",
        "# Data Scaling in (0,1)\n",
        "X = data.iloc[:, 0:data.shape[1]].values\n",
        "y = dataoutput.values\n",
        "#print(X)\n",
        "scaler = MinMaxScaler((0,1))\n",
        "scaler.fit(X)\n",
        "#scaled input data\n",
        "inputdata = scaler.transform(X)\n",
        "inputdata.shape[0] # check number of samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJzGyhjercC9"
      },
      "source": [
        "Execution of GAN runs r=1,...,N. At each iteration\n",
        "\n",
        "*   Build and train the GAN network\n",
        "*   Used the trained GAN model to generate the fake r-th dataset \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbXXUBIGriyr"
      },
      "source": [
        "\n",
        "N=10\n",
        "train_steps=5000\n",
        "for r in range(N):\n",
        "  build_and_train_models(inputdata,y,train_steps,r) #inputdata: scaled features (0,1); y target values;\n",
        "  # load the model built and trained\n",
        "  model=load_model(\"/content/drive/MyDrive/Colab Notebooks/Github XAIGAN/Real Data and Rules/training_MMH/fatigue_over\"+str(r)+\".h5\")\n",
        "  \n",
        "  # generate random noise and use the trained GAN to synthesize a fake dataset of 400 samples \n",
        "  noise_input = np.random.uniform(-1.0, 1.0, size=[400, 100])\n",
        "  noise_class = np.eye(2)[np.random.choice(2, 400)]\n",
        "  gendata = model.predict([noise_input,noise_class])\n",
        "  gendata = gendata.reshape((400,38)) # 400 samples with 38 dimensions (features)\n",
        "  gendata = scaler.inverse_transform(gendata) #reconvert to original scale\n",
        "  noise_class = tf.math.argmax(noise_class, axis=1) #reverse from one-hot format\n",
        "  # save the generated dataset to file\n",
        "  gendata = pd.DataFrame(gendata,columns=data.columns)\n",
        "  gendata['Output']=noise_class\n",
        "  gendata.to_excel(\"/content/drive/MyDrive/Colab Notebooks/Github XAIGAN/Real Data and Rules/generated_MMH/MMH_over\"+str(r)+\".xlsx\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}